{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import os\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "import umap\n",
    "from sklearn.manifold import Isomap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pk_dict(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        datadict = pickle.load(f)\n",
    "    return datadict\n",
    "\n",
    "def get_true_label_dict(path):\n",
    "    labels = {}\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            _ = line.split('\\t')\n",
    "            labels[_[0]] = _[1]\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "            \n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "        \n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=True):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "\n",
    "def save_vocab(vocab_path,vocab):\n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(vocab,f)\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1,data_type =\"protein\"):\n",
    "        print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "\n",
    "        for line in tqdm(texts):\n",
    "            if str(line).startswith(\">\"):\n",
    "                continue\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                if data_type == \"protein\":\n",
    "                    line = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "                    words = [e for e in line]\n",
    "                else:\n",
    "                    words = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        \n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "        # Parallel(n_jobs=48)(delayed(self.count)(line for line in texts)) # Less efficient than above\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "    \n",
    "    def from_seq2freq(self,seq)->list:\n",
    "        '''\n",
    "        @msg: 将句子转换为频率向量\n",
    "        @param:\n",
    "            seq :word list\n",
    "        @return:\n",
    "            list[int] :each word frequency\n",
    "        '''\n",
    "        words = seq.strip().split(\" \")\n",
    "        vocab_len = len(self.stoi)-5 \n",
    "        freq_list = [0 for _ in range(vocab_len)]\n",
    "        for w in words:\n",
    "            freq_list[self.stoi[w]-5] += 1\n",
    "    \n",
    "        return freq_list\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def build():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-t\", \"--data_type\",  type=str,default=\"dna\")\n",
    "    parser.add_argument(\"-c\", \"--corpus_path\", default =None, type=str)\n",
    "    parser.add_argument(\"-o\", \"--output_path\",  default =None, type=str)\n",
    "    parser.add_argument(\"-s\", \"--vocab_size\", type=int, default=None)\n",
    "    parser.add_argument(\"-e\", \"--encoding\", type=str, default=\"utf-8\")\n",
    "    parser.add_argument(\"-m\", \"--min_freq\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "  \n",
    "    with open(args.corpus_path, \"r\", encoding=args.encoding) as f:\n",
    "        vocab = WordVocab(f, max_size=args.vocab_size, min_freq=args.min_freq,data_type = args.data_type)\n",
    "        print(\"VOCAB SIZE:\", len(vocab))\n",
    "        \n",
    "        vocab.save_vocab(args.output_path)\n",
    "\n",
    "\n",
    "def load_pk_dict(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        datadict = pickle.load(f)\n",
    "    return datadict\n",
    "\n",
    "class FaFrequency:\n",
    "    def __init__(self,data,data_path,vocab_path,encoding=\"utf-8\"):\n",
    "        '''\n",
    "        @msg:计算每个kmer的频率\n",
    "        @param:\n",
    "            data:[msg,seq]\n",
    "            data_path:contig path\n",
    "        @return:\n",
    "            self.data[msg,freq_data]\n",
    "        '''\n",
    "        vocab = WordVocab.load_vocab(vocab_path=vocab_path)\n",
    "        self.freqs = []\n",
    "        self.seq_names = []\n",
    "        self.data = defaultdict(list)\n",
    "        if data!=None:\n",
    "            for msg,seq in data:\n",
    "                self.seq_names.append(msg)\n",
    "                freq_list = vocab.from_seq2freq(seq)\n",
    "                total = sum(freq_list)\n",
    "                for i,e in enumerate(freq_list):\n",
    "                    freq_list[i] = round(e/total,4)\n",
    "                self.freqs.append(freq_list)\n",
    "        else:\n",
    "            with open(data_path,\"r\",encoding=encoding) as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line.startswith(\">\"):\n",
    "                        self.seq_names.append(line)\n",
    "                    else:\n",
    "                        freq_list =vocab.from_seq2freq(line)\n",
    "                        total = sum(freq_list)\n",
    "                        for i,e in enumerate(freq_list):\n",
    "                            freq_list[i] = round(e/total,4)\n",
    "                        self.freqs.append(freq_list)\n",
    "        for freq,name in zip(self.freqs,self.seq_names):\n",
    "            self.data[name] = freq\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, hidden = 64,data: dict = None, nfiles: int = 111404, data_path: str = None, true_labels:dict=None,file_key: str = \"frag_embed\",  concat_mode: int = 1, cluster_method: str = \"kmeans\", partition=\"gi\",reduced=False):\n",
    "        '''\n",
    "        @msg: 在CPU上对以获取的embed数据进行聚类\n",
    "        @param:\n",
    "            concat_mode：int 对输入数据所做的后续处理\n",
    "                0:直接对数据进行聚类\n",
    "                1：先将fragment embed利用data_concat进行拼接，获取contigs的embed,再聚类\n",
    "\n",
    "            cluster_method:\n",
    "                kmeans:kmeans聚类\n",
    "                iter: vamb 启发式聚类算法\n",
    "        @return:\n",
    "        '''\n",
    "        super(Cluster).__init__()\n",
    "        assert data != data_path\n",
    "\n",
    "        self.embed = []\n",
    "        self.labels = []\n",
    "        self.seq_names = []\n",
    "        self.seq_name2label = true_labels\n",
    "        \n",
    "        if data:\n",
    "            self.embed = list(data.values())\n",
    "            for k in data.keys():\n",
    "                self.seq_names.append(k.strip().split(\"|\")[-1])\n",
    "                self.labels.append(self.seq_name2label[k.strip().split(\"|\")[-1]])\n",
    "        else:\n",
    "            \n",
    "            for i in tqdm(range(nfiles)):\n",
    "                path = os.path.join(data_path, file_key+str(i)+\".pk\")\n",
    "                if not os.path.exists(path):\n",
    "                    continue\n",
    "                datadict = load_pk_dict(path)\n",
    "                for k, v in datadict.items():\n",
    "\n",
    "                    msg = k.strip().split(\"|\")\n",
    "                    if partition!= None:\n",
    "                        if msg[3] == partition:\n",
    "                            self.embed.append(np.array(v,np.float32))\n",
    "                            self.seq_names.append(msg[-1])\n",
    "            \n",
    "                            if true_labels!=None:\n",
    "                                self.labels.append(true_labels[msg[-1]])\n",
    "                            else:\n",
    "                                self.labels.append(msg[-1])\n",
    "                    else:\n",
    "                        self.embed.append(np.array(v,np.float32))\n",
    "                        contig_id = \"|\".join([e for e in msg[-3:]])\n",
    "                        self.seq_names.append(contig_id)\n",
    "\n",
    "                        if true_labels!=None:\n",
    "                            self.labels.append(true_labels[contig_id])\n",
    "                        else:\n",
    "                            self.labels.append(contig_id)\n",
    "\n",
    "        self.concat_mode = concat_mode\n",
    "        self.cluster_method = cluster_method\n",
    "        self.hidden = hidden\n",
    "\n",
    "        self.reduced = reduced\n",
    "\n",
    "        for e in self.embed:\n",
    "            if (e==1).all():\n",
    "                print(\"1-DATA ERROR\")\n",
    "\n",
    "    def data_concat(self,max_fragment_num=10):\n",
    "        '''\n",
    "        @msg: \n",
    "            先利用已预训练好的模型对数据进行kmer嵌入，获取每个Kmer的嵌入向量，再Kmer的嵌入向量->fragment的嵌入向量->Contig的嵌入向量。\n",
    "            Kmer嵌入向量（kmers*hidden)-> Fragment嵌入向量(1*hidden)\n",
    "                由BERTPooler类实现，有多种pooling策略：MAX,CLS,MEAN\n",
    "            （self.data 为Fragment嵌入向量）\n",
    "            Fragment嵌入向量-> Contig嵌入向量（关键）\n",
    "                需要先获取所有的Fragment嵌入向量，再通过拼接策略，拼接为Contig的嵌入向量\n",
    "                拼接策略：\n",
    "                    1. 横向拓展策略[Fragment1 Fragment2 ....Fragmentm padding] (1*d)\n",
    "                        按照Fragment在切割前的顺序做横向拼接，拼接成固定维度d的一维向量，需要确定合适的向量维度d 以至于平衡舍弃的Fragment和padding\n",
    "                    2. 先纵向拓展再自乘A = [[Fragment1],[Fragment2],...,[Fragmentm]] R = A^T*A(hidden*hidden)\n",
    "                        按照Fragment在切割前的顺序做纵向拼接，无需padding，需要设计合理的聚类算法对矩阵进行聚类。\n",
    "                        存在时空消耗过大的问题\n",
    "                    3. 纵向拓展策略[[Fragment1],[Fragment2],...,[Fragmentm],padding] (d*hidden)\n",
    "                        按照Fragment在切割前的顺序做纵向拼接，拼接成d*hidden的矩阵，需要确定d,以及设计合理的聚类算法对矩阵进行聚类。\n",
    "                        存在时空消耗过大的问题\n",
    "        @param:\n",
    "            self.embed :list of fragment embeds\n",
    "            self.seq_names: list of fragment name\n",
    "            self.labels: list of fragment describe\n",
    "        @return:\n",
    "            self.embed: list of numpy array contig embeds\n",
    "            self.labels: list of contig describe\n",
    "        '''\n",
    "        contig_embed = defaultdict(list)\n",
    "\n",
    "        if self.concat_mode == 1:\n",
    "            #将相同seq_names的序列按照上述策略1进行拼接\n",
    "            for n,embed in zip(self.seq_names,self.embed):\n",
    "                embed = list(embed)\n",
    "                if n in contig_embed.keys():\n",
    "                    if len(contig_embed[n])>=max_fragment_num*self.hidden:\n",
    "                        continue\n",
    "                    else:\n",
    "                        contig_embed[n].extend(embed)\n",
    "                else:\n",
    "                    contig_embed[n] = embed\n",
    "    \n",
    "            for n,embed in contig_embed.items():\n",
    "                if len(contig_embed[n]) < max_fragment_num*self.hidden:\n",
    "                    arr = [0 for _ in range(max_fragment_num*self.hidden - len(contig_embed[n]))]\n",
    "                    embed.extend(arr)\n",
    "                \n",
    "                contig_embed[n] = np.array(embed,np.float32)\n",
    "                assert len(embed) == max_fragment_num*self.hidden\n",
    " \n",
    "        elif self.concat_mode == 2:\n",
    "            # 将相同seq_names的序列嵌入按照维度进行求平均\n",
    "            contig_cnt = {}\n",
    "            for n,embed in zip(self.seq_names,self.embed):\n",
    "\n",
    "                if n in contig_embed.keys():\n",
    "        \n",
    "                    contig_embed[n] = np.add(contig_embed[n],embed)\n",
    "                 \n",
    "\n",
    "                    # print(contig_embed[n])\n",
    "                    contig_cnt[n] += 1\n",
    "                else:\n",
    "                    contig_embed[n] = embed\n",
    "                    contig_cnt[n] = 1\n",
    "            new_contig_embed = defaultdict()\n",
    "            for n,embed in contig_embed.items():\n",
    "                new_contig_embed[n] = np.array(embed/contig_cnt[n],np.float32)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        return new_contig_embed\n",
    "\n",
    "    def reduction(self, data,dim=32, method=\"UMAP\"):\n",
    "        '''\n",
    "            @msg:利用降维方法对数据进行降维\n",
    "            @param:\n",
    "                self.embed\n",
    "            @return:\n",
    "                self.reduced_embed\n",
    "        '''\n",
    "        npdata = np.array(self.embed)\n",
    "        samples, ndim = npdata.shape\n",
    "        npdata = npdata.astype('float32')\n",
    "\n",
    "        if method == \"isomap\":\n",
    "            print(\"\\tUsing Isomap reduction!\")\n",
    "            isomap = Isomap(n_neighbors=20, n_components=dim)\n",
    "            npdata = isomap.fit_transform(npdata)\n",
    "            npdata = np.ascontiguousarray(npdata)\n",
    "            npdata = npdata.astype('float32')\n",
    "        else:\n",
    "            print(\"\\tUsing DenseMap reduction!\")\n",
    "            reducer = umap.UMAP(densmap=True, n_components=dim)\n",
    "            npdata = reducer.fit_transform(npdata)\n",
    "            npdata = np.ascontiguousarray(npdata)\n",
    "            npdata = npdata.astype('float32')\n",
    "\n",
    "        # L2 normalization\n",
    "            row_sums = np.linalg.norm(npdata, axis=1)\n",
    "            npdata = npdata / row_sums[:, np.newaxis]\n",
    "        \n",
    "        return npdata\n",
    "\n",
    "\n",
    "    def get_pred2true_label_dict(self,pred_labels):\n",
    "        pred2true_label_dict = defaultdict(list)\n",
    "        for p, t in zip(pred_labels, self.labels):\n",
    "            pred2true_label_dict[p].append(t)\n",
    "        return pred2true_label_dict\n",
    "\n",
    "    def cac_precision_recall(self, pred2true_label_dict, n):\n",
    "        \"\"\"\n",
    "        @msg:\n",
    "            计算每个聚簇的准确率和计算每个聚簇的completeness(recall)\n",
    "            定义：recall = (预测为1且正确预测的样本数)/ (所有真实情况为1的样本数) \n",
    "            定义：precision =（预测为1且正确预测的样本数）/(所有预测为1的样本数)\n",
    "        @param:\n",
    "            pred_labels:list，根据self.embed的顺序得到的预测labels\n",
    "        @return:\n",
    "            each_cluster_pre_rec:根据每个预测的label（cluster)，得到每个cluster的precision和recall指标\n",
    "                {\n",
    "                    key(predict cluster):[precision,recall]\n",
    "                }\n",
    "\n",
    "        \"\"\"\n",
    "        # 给每个聚类簇分配一个类别，这个类别是在该簇中出现次数最多\n",
    "        true_counter = dict(Counter(self.labels)) # 每个真实label的样本数量\n",
    "        for k,v in true_counter.items():\n",
    "            true_counter[k] = int(v)\n",
    "\n",
    "        corr = total_pred_sample = total_true_sample = 0\n",
    "\n",
    "        each_cluster_pre_rec = defaultdict(list)\n",
    "\n",
    "        for k, v in pred2true_label_dict.items():\n",
    "            label_counter_ = dict(Counter(v))  # list[(key,cnt)]\n",
    "    \n",
    "            sort_label_counter_ = dict(sorted(label_counter_.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "            index = 0\n",
    "            # 取Top n作为命中点\n",
    "            for k_,v_ in sort_label_counter_.items():\n",
    "                if index < n:\n",
    "                    corr += v_\n",
    "                    # 真实label为__[i][0]的所有样本数量和\n",
    "                    total_true_sample += true_counter[k_]\n",
    "                total_pred_sample += v_ # 预测的label为k的所有样本数量和\n",
    "\n",
    "            each_cluster_pre_rec[k] = [\n",
    "                round(corr/total_pred_sample, 2), round(corr/total_true_sample)]\n",
    "\n",
    "        return each_cluster_pre_rec\n",
    "\n",
    "    def cac_NC_genome(self, pred_labels: list = None,top_n:int = 5):\n",
    "        '''\n",
    "        @msg:\n",
    "            NC genome的定义为：(>90% recall and >95% precision的聚簇），计算聚类的NC数目作为最终评判指标\n",
    "        @param:\n",
    "        @return:\n",
    "        '''\n",
    "        pred2true_label_dict = self.get_pred2true_label_dict(pred_labels)\n",
    "        each_cluster_pre_rec = self.cac_precision_recall(pred2true_label_dict,top_n)\n",
    "\n",
    "        NCgene_cnt = 0\n",
    "        preformance = defaultdict(int)\n",
    "\n",
    "        for k, nc in each_cluster_pre_rec.items():\n",
    "            pre, rec = nc[0], nc[1]\n",
    "            if pre >= 0.95 and rec >= 0.9:\n",
    "                NCgene_cnt += 1\n",
    "                preformance[\"level1\"] += 1\n",
    "            elif pre>0.85 and rec >=0.8:\n",
    "                preformance[\"level2\"] += 1\n",
    "            elif pre>0.75 and rec >=0.7:\n",
    "                preformance[\"level3\"] += 1   \n",
    "            elif pre>0.65 and rec >=0.6:\n",
    "                preformance[\"level3\"] += 1   \n",
    "            elif pre>0.55 and rec >=0.5:\n",
    "                preformance[\"level3\"] += 1 \n",
    "            else:\n",
    "                preformance[\"level4\" ] +=1\n",
    "            \n",
    "\n",
    "        return NCgene_cnt,preformance\n",
    "\n",
    "    def get_rand_index_and_f_measure(self,pred_labels, beta=1.):\n",
    "        (tn, fp), (fn, tp) = pair_confusion_matrix(self.labels, pred_labels)\n",
    "        ri = (tp + tn) / (tp + tn + fp + fn)\n",
    "        ari = 2. * (tp * tn - fn * fp) / ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn))\n",
    "        p, r = tp / (tp + fp), tp / (tp + fn)\n",
    "        f_beta = (1 + beta**2) * (p * r / ((beta ** 2) * p + r))\n",
    "        purity = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "\n",
    "        return purity,recall,ri, ari, f_beta\n",
    "\n",
    "    def kmeans(self, data,k):\n",
    "        '''\n",
    "        @msg:\n",
    "            对embed采用kmeans方法聚类\n",
    "        @param:\n",
    "            k:聚簇数目\n",
    "        @return:\n",
    "            pred_labels: list 聚类结果\n",
    "        '''\n",
    "        X = StandardScaler().fit_transform(self.embed)\n",
    "        cl = cluster.MiniBatchKMeans(n_clusters=k).fit(X)\n",
    "        return cl.labels_\n",
    "\n",
    "    def iter_cluster(self,data):\n",
    "        pass\n",
    "\n",
    "    def cluster_data(self, k: int = 0, top_n: int = 5,dim = 32,reduced_method = 'umap'):\n",
    "        '''\n",
    "        @msg:\n",
    "            按照配置对数据进行聚类\n",
    "        @param:\n",
    "        @return:\n",
    "            NC genome的数量\n",
    "            每个预测的聚类（genome)的precision和recall指标\n",
    "        '''\n",
    "        embeds = self.embed\n",
    "        seq_names = self.seq_names\n",
    "        contig_embed = None\n",
    "\n",
    "        if self.concat_mode > 0:\n",
    "            contig_embed = self.data_concat()\n",
    "            print(\"\\tThere have {} samples\".format(len(self.embed)))\n",
    "        \n",
    "            embeds = list(contig_embed.values())\n",
    "            seq_names = list(contig_embed.keys())\n",
    "\n",
    "        for e in embeds:\n",
    "            if (e==1).all():\n",
    "                print(\"2-DATA ERROR\")\n",
    "      \n",
    "        ndim = len(embeds[0])\n",
    "\n",
    "        if ndim > 32 and self.reduced:\n",
    "            embeds = self.reduction(embeds,dim=dim,method=reduced_method)\n",
    "            print(\"\\tThe dim from {} reduced to  {}\".format(ndim,embeds[0].shape))\n",
    "\n",
    "        if self.cluster_method == \"kmeans\":\n",
    "            pred_labels = self.kmeans(embeds,k)\n",
    "\n",
    "        elif self.cluster_method == \"iter\":\n",
    "            pred_labels = self.iter_cluster(embeds)\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        NCgene_cnt,preformance  = self.cac_NC_genome(\n",
    "                pred_labels, top_n)\n",
    "        purity,recall,ri, ari, f_measure  = self.get_rand_index_and_f_measure(pred_labels)\n",
    "        print(\"NC genome:\", NCgene_cnt)\n",
    "        print(\"NC genome rate :\",NCgene_cnt/len(pred_labels))\n",
    "        print(\"Purity :\",purity)\n",
    "        print(\"Recall :\",recall)\n",
    "        print(\"RI :\", ri)\n",
    "        print(\"ARI ：\" ,ari)\n",
    "        print(\"F_measure :\",f_measure)\n",
    "        print(preformance)\n",
    "\n",
    "        return  NCgene_cnt,preformance ,embeds,seq_names\n",
    "    \n",
    "\n",
    "def get_true_label_dict(path):\n",
    "    labels = {}\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            _ = line.split('\\t')\n",
    "            labels[_[0]] = _[1]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/workspace/MG-DL/datasets/toy-low/1106/encode-512-max\"\n",
    "\n",
    "\n",
    "label_path = \"/workspace/MG-DL/datasets/toy-low/gsa_mapping.tsv\"\n",
    "\n",
    "labels = get_true_label_dict(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(labels.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1749/1749 [00:00<00:00, 6669.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThere have 13984 samples\n",
      "\tUsing DenseMap reduction!\n",
      "\tThe dim from 512 reduced to  (32,)\n",
      "NC genome: 0\n",
      "NC genome rate : 0.0\n",
      "Purity : 0.17435784570673515\n",
      "Recall : 0.19934679754254642\n",
      "RI : 0.9241918635754335\n",
      "ARI ： 0.14644791924066144\n",
      "F_measure : 0.18601683867690022\n",
      "defaultdict(<class 'int'>, {'level4': 30})\n"
     ]
    }
   ],
   "source": [
    "cl = Cluster(hidden = 512,data=None, nfiles=1749, data_path=data_path,true_labels=labels,\n",
    "                file_key=\"frag_embed\",  concat_mode=2, cluster_method=\"kmeans\",reduced =True,partition=None)\n",
    "\n",
    "# cl = Cluster(data=None, nfiles=10, data_path=data_path,true_labels=labels,\n",
    "#                 file_key=\"frag_embed\",  concat_mode=2, cluster_method=\"kmeans\",reduced =False,partition='gi')\n",
    "NCgene_cnt,preformance,embeds,seq_names = cl.cluster_data(k=30,top_n=1,dim=32,reduced_method='umap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_npz(file,data):\n",
    "    np.savez_compressed(file,data)\n",
    "\n",
    "def validate_input_array(array):\n",
    "    \"Returns array similar to input array but C-contiguous and with own data.\"\n",
    "    if not array.flags['C_CONTIGUOUS']:\n",
    "        array = np.ascontiguousarray(array)\n",
    "    if not array.flags['OWNDATA']:\n",
    "        array = array.copy()\n",
    "\n",
    "    assert (array.flags['C_CONTIGUOUS'] and array.flags['OWNDATA'])\n",
    "    return array\n",
    "\n",
    "def load_npz(file,allow_pickle=True):\n",
    "    npz = np.load(file,allow_pickle=allow_pickle)\n",
    "    array = validate_input_array(npz['arr_0'])\n",
    "    npz.close()\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_npz(\"/workspace/MG-DL/datasets/toy-low/1106/embeds-512.npz\",list(embeds))\n",
    "write_npz(\"/workspace/MG-DL/datasets/toy-low/1106/embeds-512-names.npz\",list(seq_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3061035730dba9d0b680da3611b12e6f8d02ffef1a4209a1862fc890317943a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('clbert': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
